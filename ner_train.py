import torch
import torch
import random
import numpy as np
import pandas as pd
import os
import glob
import torch.backends.cudnn as cudnn

torch.manual_seed(42)
torch.cuda.manual_seed(42)
torch.cuda.manual_seed_all(42)
np.random.seed(42)
cudnn.benchmark = False
cudnn.deterministic = True
random.seed(42)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

!git clone https://github.com/kmounlp/NER.git

file_list = []
for x in os.walk('NER/'):
    for y in glob.glob(os.path.join(x[0], '*_NER.txt')):    # ner.*, *_NER.txt
        file_list.append(y)

file_list = sorted(file_list)

!pip install korpora

#NAVER NLP Challenge 2018 Dataset
from Korpora import Korpora
corpus = Korpora.load("naver_changwon_ner")

#í•œêµ­ í•´ì–‘ëŒ€í•™êµ ë°ì´í„°ì…‹
from pathlib import Path

file_path = file_list[0]
file_path = Path(file_path)
raw_text = file_path.read_text().strip()
#ë„¤ì´ë²„ NER ì±Œë¦°ì§€ ë°ì´í„°ì…‹
corpus.train[0:10]

org = pd.read_csv('/kaggle/input/asgasbsa/.csv',)
org_df = org.iloc[:,0:1]

#ìœ ëª…ì¸ ì´ë¦„ ë°ì´í„°ì…‹
per_df = pd.read_csv('/kaggle/input/per-list/per_name.csv')
per_df = per_df.drop(['Unnamed: 0'], axis = 1)
per_df

def naver_read_file(file_list):


    token_docs = []
    tag_docs = []

    for doc in file_list:
        tokens = []
        tags = []
        list1=doc.words
        list2=doc.tags


        # BIO íƒœê¹… ë°©ì‹ì€ í•œêµ­ í•´ì–‘ëŒ€í•™êµ ìžì—°ì–´ì²˜ë¦¬ ì—°êµ¬ì‹¤ ë°ì´í„° ì…‹ì˜ ë°©ì‹ìœ¼ë¡œ í†µì¼í•˜ê¸° ìœ„í•´
        # ì•„ëž˜ì²˜ëŸ¼ íƒœê¹… ë°©ì‹ì„ ë³€ê²½
        for text,docs in zip(list1,list2):
            try:
                tag = docs
                if tag == 'ORG_B':
                    tag='B-ORG'
                elif tag == 'PER_B':
                    tag ='B-PER'
                elif tag == 'FLD_B':
                    tag ='B-FLD'
                elif tag == 'AFW_B':
                    tag ='B-AFW'
                elif tag == 'LOC_B':
                    tag ='B-LOC'
                elif tag == 'CVL_B':
                    tag ='B-CVL'
                elif tag == 'DAT_B':
                    tag ='B-DAT'
                elif tag == 'TIM_B':
                    tag ='B-TIM'
                elif tag == 'NUM_B':
                    tag ='B-NUM'
                elif tag == 'EVT_B':
                    tag ='B-EVT'
                elif tag == 'ANM_B':
                    tag ='B-ANM'
                elif tag == 'PLT_B':
                    tag ='B-PLT'
                elif tag == 'MAT_B':
                    tag ='B-MAT'
                elif tag == 'TRM_B':
                    tag ='B-TRM'
                else:
                    tag = 'O'

                if tag in ['B-PER', 'B-DAT', 'B-LOC', 'B-ORG']:
                    if tag == 'B-ORG':
                        token = random.sample(org_df['íšŒì‚¬ëª…'].tolist(), k=1)[0] #tagê°€ B-ORGì´ë©´ ìƒìž¥ë²•ì¸ëª©ë¡ì˜ íšŒì‚¬ëª… ì¤‘ í•˜ë‚˜ë¡œ ëžœë¤ìœ¼ë¡œ ëŒ€ì²´
                    elif tag == 'B-PER':
                        token = random.sample(per_df['ì´ë¦„'].tolist(), k=1)[0] #tagê°€ B-PERì´ë©´ ìœ ëª…ì¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ì¤‘ í•˜ë‚˜ë¡œ ëŒ€ì²´
                    else:
                        token = text
                else:
                    token = text
                    tag = 'O'

                for i, syllable in enumerate(token): # ìŒì ˆ ë‹¨ìœ„ë¡œ ìžë¥´ê³ 
                    tokens.append(syllable)
                    modi_tag = tag
                    if i > 0:
                        if tag[0] == 'B':
                            modi_tag = 'I' + tag[1:]     # BIO tagë¥¼ ë¶€ì°©
                    tags.append(modi_tag)
            except:
                continue
        token_docs.append(tokens)
        tag_docs.append(tags)

    return token_docs, tag_docs

naver_text,naver_tags = naver_read_file(corpus.train)
import re


def read_file(file_list):  # í•œêµ­ í•´ì–‘ëŒ€í•™êµ
    token_docs = []
    tag_docs = []
    for file_path in file_list:
        # print("read file from ", file_path)
        file_path = Path(file_path)
        raw_text = file_path.read_text().strip()
        raw_docs = re.split(r'\n\t?\n', raw_text)
        for doc in raw_docs:
            tokens = []
            tags = []
            for line in doc.split('\n'):
                if line[0:1] == "$" or line[0:1] == ";" or line[0:2] == "##":
                    continue
                try:
                    tag = line.split('\t')[3]  # 2: pos, 3: ner
                    if tag in ['B-PER', 'B-DAT', 'B-LOC', 'B-ORG']:
                        if tag == 'B-ORG':
                            token = random.sample(org_df['íšŒì‚¬ëª…'].tolist(), k=1)[0]
                        elif tag == 'B-PER':
                            token = random.sample(per_df['ì´ë¦„'].tolist(), k=1)[0]
                        else:
                            token = line.split('\t')[0]

                        # elif 'I-' in tag:
                    elif tag in ['I-PER', 'I-DAT', 'I-LOC', 'I-ORG']:
                        if tag == 'I-ORG':
                            token = None
                        if tag == 'I-PER':
                            token = None
                        else:
                            token = line.split('\t')[0]
                    else:
                        token = line.split('\t')[0]
                        tag = 'O'

                    # token = line.split('\t')[0]

                    for i, syllable in enumerate(token):  # ìŒì ˆ ë‹¨ìœ„ë¡œ
                        tokens.append(syllable)
                        modi_tag = tag
                        if i > 0:
                            if tag[0] == 'B':
                                modi_tag = 'I' + tag[1:]  # BIO tagë¥¼ ë¶€ì°©
                        tags.append(modi_tag)
                except:
                    continue
            token_docs.append(tokens)
            tag_docs.append(tags)

    return token_docs, tag_docs

texts, tags = read_file(file_list[:])

texts.extend(naver_text)
tags.extend(naver_tags)

unique_tags = set(tag for doc in tags for tag in doc)
tag2id = {tag: id for id, tag in enumerate(list(unique_tags))}
id2tag = {id: tag for tag, id in tag2id.items()}

from sklearn.model_selection import train_test_split
train_texts, test_texts, train_tags, test_tags = train_test_split(texts, tags, test_size=.2, random_state=42)

#ì—¬ê¸°ë¶€í„° NER ëª¨ë¸ í•™ìŠµ ì½”ë“œ

from transformers import AutoModel, AutoTokenizer, BertTokenizer
MODEL_NAME = "beomi/KcELECTRA-base-v2022"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

tag2id = {'B-LOC': 0,
 'I-PER': 1,
 'I-ORG': 2,
 'O': 3,
 'B-DAT': 4,
 'I-DAT': 5,
 'B-ORG': 6,
 'I-LOC': 7,
 'B-PER': 8}
unique_tags={'B-DAT',
 'B-LOC',
 'B-ORG',
 'B-PER',
 'I-DAT',
 'I-LOC',
 'I-ORG',
 'I-PER',
 'O'}
id2tag={0: 'B-LOC',
 1: 'I-PER',
 2: 'I-ORG',
 3: 'O',
 4: 'B-DAT',
 5: 'I-DAT',
 6: 'B-ORG',
 7: 'I-LOC',
 8: 'B-PER'}

pad_token_id = tokenizer.pad_token_id # 0
cls_token_id = tokenizer.cls_token_id # 101
sep_token_id = tokenizer.sep_token_id # 102
pad_token_label_id = tag2id['O']    # tag2id['O']
cls_token_label_id = tag2id['O']
sep_token_label_id = tag2id['O']

def ner_tokenizer(sent, max_seq_length):
    pre_syllable = "_"
    input_ids = [pad_token_id] * (max_seq_length - 1)
    attention_mask = [0] * (max_seq_length - 1)
    token_type_ids = [0] * max_seq_length
    sent = sent[:max_seq_length-2]

    for i, syllable in enumerate(sent):
        if syllable == '_':
            pre_syllable = syllable
        if pre_syllable != "_":
            syllable = '##' + syllable  # ì¤‘ê°„ ìŒì ˆì—ëŠ” ëª¨ë‘ prefixë¥¼ ë¶™ìž…ë‹ˆë‹¤.
            # ìš°ë¦¬ê°€ êµ¬ì„±í•œ í•™ìŠµ ë°ì´í„°ë„ ì´ë ‡ê²Œ êµ¬ì„±ë˜ì—ˆê¸° ë•Œë¬¸ì´ë¼ê³  í•¨.
            # ì´ìˆœì‹ ì€ ì¡°ì„  -> [ì´, ##ìˆœ, ##ì‹ , ##ì€, ì¡°, ##ì„ ]
        pre_syllable = syllable

        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))
        attention_mask[i] = 1

    input_ids = [cls_token_id] + input_ids
    input_ids[len(sent)+1] = sep_token_id
    attention_mask = [1] + attention_mask
    attention_mask[len(sent)+1] = 1
    return {"input_ids":input_ids,
            "attention_mask":attention_mask,
            "token_type_ids":token_type_ids}

tokenized_train_sentences = []
tokenized_test_sentences = []

for text in train_texts:    # ì „ì²´ ë°ì´í„°ë¥¼ tokenizing í•©ë‹ˆë‹¤.
    tokenized_train_sentences.append(ner_tokenizer(text, 128))
for text in test_texts:
    tokenized_test_sentences.append(ner_tokenizer(text, 128))

def encode_tags(tags, max_seq_length):
    # label ì—­ì‹œ ìž…ë ¥ tokenê³¼ ê°œìˆ˜ë¥¼ ë§žì¶°ì¤ë‹ˆë‹¤
    tags = tags[:max_seq_length-2]
    labels = [tag2id[tag] for tag in tags]
    labels = [tag2id['O']] + labels

    padding_length = max_seq_length - len(labels)
    labels = labels + ([pad_token_label_id] * padding_length)

    return labels

train_labels = []
test_labels = []

for tag in train_tags:
    train_labels.append(encode_tags(tag, 128))

for tag in test_tags:
    test_labels.append(encode_tags(tag, 128))

import torch

class TokenDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = TokenDataset(tokenized_train_sentences, train_labels)
test_dataset = TokenDataset(tokenized_test_sentences, test_labels)

import accelerate
import transformers

transformers.__version__, accelerate.__version__

# BertForSencenceClassificationì´ ì•„ë‹ˆë‹¤! tokenì´ ëª©ì ì´ë‹¤.
from transformers import BertForTokenClassification, Trainer, TrainingArguments, AutoModelForTokenClassification,EarlyStoppingCallback
import sys
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    logging_dir='./logs',            # directory for storing logs
    logging_steps=1000, # 1000ë²ˆì¨° stepsë§ˆë‹¤ logë¥¼ ë³´ì—¬ì¤Œ
    learning_rate=3e-5,
    weight_decay=0.01,
    save_total_limit=5,
    save_strategy='steps', # stepsë¡œ í•´ì•¼ earlystopì´ ê°€ëŠ¥
    evaluation_strategy='steps',
    save_steps=1000, # 1000ë²ˆì¨° stepë§ˆë‹¤ ì €ìž¥
    eval_steps=1000, # 1000ë²ˆì§¸ stepë§ˆë‹¤ í‰ê°€
    seed=15,
    load_best_model_at_end=True # ê°€ìž¥ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ë¡œ...
)

model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))
model.to(device)

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset,# evaluation dataset
    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)] #lossê°€ 2ë²ˆ ê°ì†Œí•˜ì§€ ì•Šìœ¼ë©´ ìŠ¤íƒ‘
)

import gc
gc.collect()

trainer.train()
trainer.evaluate()

predictions = trainer.predict(test_dataset)
print(predictions.predictions.shape, predictions.label_ids.shape)

preds = np.argmax(predictions.predictions, axis=-1)
index_to_ner = {i:j for j, i in tag2id.items()}
f_label = [i for i, j in tag2id.items()]
val_tags_l = [index_to_ner[x] for x in np.ravel(predictions.label_ids).astype(int).tolist()]
y_predicted_l = [index_to_ner[x] for x in np.ravel(preds).astype(int).tolist()]

from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

print(classification_report(val_tags_l, y_predicted_l, labels=f_label))

trainer.save_model('ner_model')

# ì €ìž¥í•œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

from transformers import AutoModel, AutoTokenizer, BertTokenizer
MODEL_NAME = "beomi/KcELECTRA-base-v2022"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
unique_tags={'B-DAT',
 'B-LOC',
 'B-ORG',
 'B-PER',
 'I-DAT',
 'I-LOC',
 'I-ORG',
 'I-PER',
 'O'}
#tag2idì™€ id2tagëŠ” í•™ìŠµí•˜ë©° ì§€ì •ëœ ê·¸ëŒ€ë¡œ ì‚¬ìš©

# tag2id = {'B-LOC': 0,
#  'I-PER': 1,
#  'I-ORG': 2,
#  'O': 3,
#  'B-DAT': 4,
#  'I-DAT': 5,
#  'B-ORG': 6,
#  'I-LOC': 7,
#  'B-PER': 8}
# id2tag={0: 'B-LOC',
#  1: 'I-PER',
#  2: 'I-ORG',
#  3: 'O',
#  4: 'B-DAT',
#  5: 'I-DAT',
#  6: 'B-ORG',
#  7: 'I-LOC',
#  8: 'B-PER'}

pad_token_id = tokenizer.pad_token_id # 0
cls_token_id = tokenizer.cls_token_id # 101
sep_token_id = tokenizer.sep_token_id # 102
pad_token_label_id = tag2id['O']    # tag2id['O']
cls_token_label_id = tag2id['O']
sep_token_label_id = tag2id['O']

model = AutoModelForTokenClassification.from_pretrained('/kaggle/input/ner-per-comp', num_labels=len(unique_tags))
model.to(device)

# ê¸°ì¡´ í† í¬ë‚˜ì´ì €ëŠ” wordPiece tokenizerë¡œ tokenizing ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
# ë°ì´í„° ë‹¨ìœ„ë¥¼ ìŒì ˆ ë‹¨ìœ„ë¡œ ë³€ê²½í–ˆê¸° ë•Œë¬¸ì—, tokenizerë„ ìŒì ˆ tokenizerë¡œ ë³€ê²½

# berttokenizerë¥¼ ì‚¬ìš©í•˜ëŠ”ë° í•œêµ­ì–´ vocabì´ 8000ê°œ ì •ë„ ë°–ì— ì—†ê³  ê·¸ ì•ˆì˜ í•œêµ­ì–´ë“¤ì˜ ê±°ì˜ ìŒì ˆë¡œ ì¡´ìž¬
# -> ìŒì ˆ ë‹¨ìœ„ tokenizerë¥¼ ì ìš©í•˜ë©´ vocab idë¥¼ ì–´ëŠ ì •ë„ íšë“í•  ìˆ˜ ìžˆì–´ UNKê°€ ë³„ë¡œ ì—†ì„ë“¯ í•˜ë‹¤
def ner_tokenizer(sent, max_seq_length):
    pre_syllable = "_"
    input_ids = [pad_token_id] * (max_seq_length - 1)
    attention_mask = [0] * (max_seq_length - 1)
    token_type_ids = [0] * max_seq_length
    sent = sent[:max_seq_length-2]

    for i, syllable in enumerate(sent):
        if syllable == '_':
            pre_syllable = syllable
        if pre_syllable != "_":
            syllable = '##' + syllable  # ì¤‘ê°„ ìŒì ˆì—ëŠ” ëª¨ë‘ prefixë¥¼ ë¶™ìž…ë‹ˆë‹¤.
            # ìš°ë¦¬ê°€ êµ¬ì„±í•œ í•™ìŠµ ë°ì´í„°ë„ ì´ë ‡ê²Œ êµ¬ì„±ë˜ì—ˆê¸° ë•Œë¬¸ì´ë¼ê³  í•¨.
            # ì´ìˆœì‹ ì€ ì¡°ì„  -> [ì´, ##ìˆœ, ##ì‹ , ##ì€, ì¡°, ##ì„ ]
        pre_syllable = syllable

        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))
        attention_mask[i] = 1

    input_ids = [cls_token_id] + input_ids
    input_ids[len(sent)+1] = sep_token_id
    attention_mask = [1] + attention_mask
    attention_mask[len(sent)+1] = 1
    return {"input_ids":input_ids,
            "attention_mask":attention_mask,
            "token_type_ids":token_type_ids}

#Inference
def ner_inference(text) :

    model.eval()
    text = text.replace(' ', '_')

    predictions , true_labels = [], []

    tokenized_sent = ner_tokenizer(text, len(text)+2)
    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)
    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)
    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)

    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids)

    logits = outputs['logits']
    logits = logits.detach().cpu().numpy()
    label_ids = token_type_ids.cpu().numpy()

    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
    true_labels.append(label_ids)

    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]

    print('{}\t{}'.format("TOKEN", "TAG"))
    print("===========")
    # for token, tag in zip(tokenizer.decode(tokenized_sent['input_ids']), pred_tags):
    #   print("{:^5}\t{:^5}".format(token, tag))
    for i, tag in enumerate(pred_tags):
        print("{:^5}\t{:^5}".format(tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]), tag))


text = 'ë°•ì •ê·œëŠ” 12ì›” 25ì¼ì— ë‚˜ì•„ì§€ëŠ” ì„±ëŠ¥ì„ ë³´ë©° SKê¸°ì—…ì˜ í›„ì›ì„ ë°›ëŠ” ìž„ì£¼ì›ì”¨ë¥¼ ê¸°ë‹¤ë¦¬ê³  ìžˆë‹¤.'
ner_inference(text)

text = 'ì¢…ëª©ë³„ë¡œëŠ”  ë¯¸ëž˜ì—ì…‹ì¦ê¶Œ(+2.83%)  ë©”ë¦¬ì¸ ì¦ê¶Œ(+3.99%)  í•œêµ­ê¸ˆìœµì§€ì£¼(+3.07%)  ì‚¼ì„±ì¦ê¶Œ(+2.67%)  NHíˆ¬ìžì¦ê¶Œ(+2.08)  í‚¤ì›€ì¦ê¶Œ(+4.06%) ë“±ì´ ê°•ì„¸ë¥¼ ë³´ì˜€ë‹¤. KRX ì¦ê¶Œ ì§€ìˆ˜ëŠ” ì¦ì‹œì— ìƒìž¥ëœ ì¦ê¶Œì—…ì¢…ì˜ ì£¼ê°€ íë¦„ì„ ë°˜ì˜í•˜ëŠ” ì§€ìˆ˜ë¡œ ë¯¸ëž˜ì—ì…‹ì¦ê¶Œ, í•œêµ­ê¸ˆìœµì§€ì£¼, NHíˆ¬ìžì¦ê¶Œ ë“± 14ê°œ ì¢…ëª©ì´ ì§€ìˆ˜ì— í¬í•¨ë¼ ìžˆë‹¤. ì¦ê¶Œì •ë³´ì—…ì²´ ì—í”„ì•¤ê°€ì´ë“œì— ë”°ë¥´ë©´ ì‹¤ì ì¶”ì •ì¹˜ê°€ ìžˆëŠ” ì¦ê¶Œì‚¬ ë‹¤ì„¯ êµ°ë°(ì‚¼ì„±ì¦ê¶Œ, ë¯¸ëž˜ì—ì…‹ì¦ê¶Œ, í‚¤ì›€ì¦ê¶Œ, í•œêµ­ê¸ˆìœµì§€ì£¼, NHíˆ¬ìžì¦ê¶Œ)ì˜ 4ë¶„ê¸° ì˜ì—…ì´ìµ ì „ë§ì¹˜ í•©ì€ 8558ì–µ ì›ìœ¼ë¡œ ì „ë…„ ë™ê¸°ë³´ë‹¤ 27.60% ì¤„ì–´ë“¤ ì „ë§ì´ë‹¤.'

ner_inference(text)


#ì•„ëž˜ëŠ” NER ëŒ€ìƒ ë°ì´í„° EDA

import numpy as np
import matplotlib.pyplot as plt

texts_len = [len(x) for x in texts]

plt.figure(figsize=(16,10))
plt.hist(texts_len, bins=50, range=[0,800], facecolor='b', density=True, label='Text Length')
plt.title('Text Length Histogram')
plt.legend()
plt.xlabel('Number of Words')
plt.ylabel('Probability')

#ê° NER íƒœê·¸ë³„ ë°ì´í„° ê°œìˆ˜

for tag in list(tag2id.keys()) :
    globals()[tag] = 0
for tag in tags :
    for ner in tag :
        globals()[ner] += 1
for tag in list(tag2id.keys()) :
    print('{:>6} : {:>7,}'. format(tag, globals()[tag]))



